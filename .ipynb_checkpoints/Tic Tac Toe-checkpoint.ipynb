{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exercise in reinforcement learning.  An <b>agent</b> (our AI) makes moves in an <b>environment</b> (our gameboard) using a <b>policy</b> to determine the best move in a given <b>state</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "# defining string constants for specifying policies to use\n",
    "RANDOM_POLICY = \"random_policy\"\n",
    "GREEDY_POLICY = \"greedy_policy\"\n",
    "GREEDY_BUT_TYTHING_POLICY = \"greedy_but_tything_policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment for our agent to interact with.  A gameboard for tic tac toe.\n",
    "class GameBoard:\n",
    "    def __init__(self, dim=3, grid = None):\n",
    "        if np.any(grid == None):\n",
    "            self.grid = np.zeros((dim, dim), dtype=np.int)\n",
    "            self.dim = dim\n",
    "        else:\n",
    "            self.grid = grid\n",
    "            self.dim = grid.shape[0]\n",
    "        self.game_over = False\n",
    "        self.game_has_winner = False\n",
    "        self.outcome = 0 #default value to be replaced by 1 for \"X\" or -1 for \"O\"\n",
    "\n",
    "        \n",
    "    def available_moves(self):\n",
    "        result = np.where(self.grid == 0)\n",
    "        listOfCoordinates = list(zip(result[0], result[1]))\n",
    "        return listOfCoordinates\n",
    "\n",
    "    #enter a move, 1 being \"X\", -1 being \"O\"\n",
    "    def update_state(self, coords, val):\n",
    "        self.grid[coords] = val\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.grid\n",
    "    \n",
    "    def view_state(self):\n",
    "        print(self.grid)\n",
    "        \n",
    "    def evaluate_position(self):\n",
    "        # first check if there is a winner\n",
    "        # if a row, column, or diagonal adds up to dim or -dim, it mean X or O has won, respectively\n",
    "        row_comp = np.any(np.abs(np.sum(self.grid, axis = 1)) == self.dim)\n",
    "        col_comp = np.any(np.abs(np.sum(self.grid, axis = 0)) == self.dim)\n",
    "        diag_comp = np.abs(np.trace(self.grid)) == self.dim\n",
    "        anti_diag_comp = np.abs(np.trace(np.flipud(self.grid))) == self.dim\n",
    "        if row_comp or col_comp or diag_comp or anti_diag_comp:\n",
    "            self.game_over = True\n",
    "            self.game_has_winner = True\n",
    "        # check if the grid is filled\n",
    "        # a draw, since, neither player has won by the above criteria\n",
    "        elif not np.any(self.grid == 0):\n",
    "            self.game_over = True\n",
    "            self.game_has_winner = False\n",
    "    \n",
    "    def determine_winner(self):\n",
    "        self.evaluate_position()\n",
    "        if self.game_has_winner:\n",
    "            row_comp = np.any(np.sum(self.grid, axis = 1) == self.dim)\n",
    "            col_comp = np.any(np.sum(self.grid, axis = 0) == self.dim)\n",
    "            diag_comp = np.trace(self.grid) == self.dim\n",
    "            anti_diag_comp = np.trace(np.flipud(self.grid)) == self.dim\n",
    "            if row_comp or col_comp or diag_comp or anti_diag_comp:\n",
    "                self.outcome = 1\n",
    "            else:\n",
    "                self.outcome = -1\n",
    "    \n",
    "    def result(self):\n",
    "        self.determine_winner()\n",
    "        if self.game_over and self.game_has_winner:\n",
    "            if self.outcome == 1:\n",
    "                print(\"X won!\")\n",
    "            else:\n",
    "                print(\"O won!\")\n",
    "        elif self.game_over:\n",
    "            print(\"It's a draw\")\n",
    "        else:\n",
    "            print(\"Game in progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our agent that will be playing the game\n",
    "class Agent:\n",
    "    def __init__(self, epsilon=0.1, discount_factor=0.8, learning_rate=0.2, tythe_rate = 0.5,\n",
    "                 move=1): # move=1 for X, -1 for O\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tythe_rate = tythe_rate\n",
    "        self.move = move\n",
    "        self.game_list = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game_list = []\n",
    "    \n",
    "    # deprecated\n",
    "    def calculate_rewards(self, game):\n",
    "        reward = self.move * game.outcome\n",
    "        for i in range(1, len(self.game_list)+1):\n",
    "            self.game_list[-i]['reward'] = self.discount_factor**(i-1) * reward\n",
    "    \n",
    "    # deprecated\n",
    "    def update_value_table_mc(self, game, value_table):\n",
    "        self.calculate_rewards(game)\n",
    "        for item in self.game_list:\n",
    "            key = item['new_state']\n",
    "            reward = item['reward']\n",
    "            if key not in value_table.keys():\n",
    "                value_table[key] = 0\n",
    "            value_table[key] += self.learning_rate * (reward - value_table[key])\n",
    "    \n",
    "#     # the more effective optimisation algorithm\n",
    "#     # kudos to Haron Shams for explaining this to me, and for example code\n",
    "#     def update_value_table_bellman(self, game, value_table):\n",
    "#         reward = self.move * game.outcome\n",
    "#         for i in range(1, len(self.game_list)+1):\n",
    "#             key = self.game_list[-i]['new_state']\n",
    "#             dim = game.dim\n",
    "#             if key not in value_table:\n",
    "#                 value_table[key] = 0\n",
    "#             if i == 1:\n",
    "#                 calculated_value = reward\n",
    "#             else:\n",
    "#                 calculated_value = self.discount_factor * (self.value_of_best_move(key, dim, value_table))\n",
    "#             value_table[key] += self.learning_rate * (calculated_value - value_table[key])\n",
    "\n",
    "            \n",
    "            \n",
    "    # we will adopt the convention that self.move = 1 to reduce the complexity of the following code\n",
    "    def update_value_table_bellman(self, agent, game, value_table):\n",
    "        dim = game.dim\n",
    "        game_list_X = self.game_list.copy()\n",
    "        game_list_O = agent.game_list.copy()\n",
    "        game_list = [game_list_X, game_list_O]\n",
    "        agents = [self, agent]\n",
    "        final_move = True # only the final move will not be suppressed by the discount factor\n",
    "        # determining who moved last\n",
    "        if len(game_list_X) > len(game_list_O):\n",
    "            index = 0\n",
    "            inc = -1\n",
    "        else:\n",
    "            index = 1\n",
    "            inc = 1\n",
    "        \n",
    "        while len(game_list_X) > 0 or len(game_list_O) > 0:\n",
    "            key = game_list[index][-1]['new_state']\n",
    "            current_agent = agents[index]\n",
    "            opposing_agent = agents[index-1]\n",
    "            if key not in value_table:\n",
    "                value_table[key] = 0\n",
    "            if final_move:\n",
    "                calculated_value = current_agent.move * game.outcome\n",
    "                final_move = False\n",
    "            else:\n",
    "                # a bit complicated, but we are valuing the move based on the best follow-up move by the \n",
    "                # opponent.  So the better the opponent's response, the heavier the penalty, hence the (-1) factor, since\n",
    "                # it's the opponent's move in the new state\n",
    "                calculated_value = (-1) * current_agent.discount_factor * (opposing_agent.value_of_best_move(key, dim, value_table))\n",
    "            value_table[key] += current_agent.learning_rate * (calculated_value - value_table[key])\n",
    "            del game_list[index][-1]\n",
    "            inc = -inc\n",
    "            index +=inc\n",
    "\n",
    "\n",
    "    def value_of_best_move(self, encoded_state, dim, value_table):\n",
    "        # return an array with the dimension dim and entries in the encoded state\n",
    "        grid = self.decode_state(encoded_state, dim)\n",
    "        game = GameBoard(dim=dim, grid=grid)\n",
    "        return self.greedy_policy(game, value_table, return_max_score = True)\n",
    "    \n",
    "    def decode_state(self, encoded_state, dim):\n",
    "        grid = np.zeros((dim, dim), dtype=np.int)\n",
    "        upto = encoded_state.find('-1:')\n",
    "        xs = encoded_state[3:upto]\n",
    "        os = encoded_state[upto+3:]\n",
    "        len1=len(xs)\n",
    "        len2=len(os)\n",
    "        for i in range(int(len1/2)):\n",
    "            grid[int(xs[2*i]), int(xs[2*i+1])] = 1\n",
    "        for i in range(int(len2/2)):\n",
    "            grid[int(os[2*i]), int(os[2*i+1])] = -1\n",
    "        return grid\n",
    "        \n",
    "    # value table is a dictionary of states and values.  game.get_state is a numpy array that can't be used\n",
    "    # as a dictionary key.  We encode the state as a string for it to be usable.  If the agent plays O's,\n",
    "    # we will multiply the matrix by move (-1) first, so that the state is the same for both X and O\n",
    "    def encode_state(self, game):\n",
    "        state = game.get_state()\n",
    "        xs = np.where(state == 1)\n",
    "        os = np.where(state == -1)\n",
    "        xcoords = list(zip(xs[0], xs[1]))\n",
    "        ocoords = list(zip(os[0], os[1]))\n",
    "        encoded_state = '+1:'\n",
    "        for tup in xcoords:\n",
    "            for a in tup:\n",
    "                encoded_state +=str(a)\n",
    "        encoded_state +='-1:'\n",
    "        for tup in ocoords:\n",
    "            for a in tup:\n",
    "                encoded_state +=str(a)\n",
    "        return encoded_state\n",
    "\n",
    "    def take_action(self, game, value_table, policy_string):\n",
    "        if policy_string == RANDOM_POLICY:\n",
    "            policy = self.random_policy\n",
    "        elif policy_string == GREEDY_POLICY:\n",
    "            policy = self.greedy_policy\n",
    "        elif policy_string == GREEDY_BUT_TYTHING_POLICY:\n",
    "            policy = self.greedy_but_tything_policy\n",
    "        action = policy(game, value_table)\n",
    "        game.update_state(action, self.move)\n",
    "    \n",
    "    def random_policy(self, game, value_table):\n",
    "        return r.choice(game.available_moves())\n",
    "\n",
    "    def greedy_policy(self, game, value_table, return_max_score=False):\n",
    "        available_moves = game.available_moves() # i.e. get unfilled positions in grid\n",
    "        move_scores = []\n",
    "        for available_move in available_moves:\n",
    "            game.update_state(available_move, self.move) # getting position after potential move is made\n",
    "            encoded_state = self.encode_state(game)\n",
    "            if encoded_state not in value_table.keys():\n",
    "#                 value_table[encoded_state] = 0\n",
    "                game.update_state(available_move, 0)\n",
    "                continue\n",
    "            move_scores.append(value_table[encoded_state]) # getting value of new position\n",
    "            game.update_state(available_move, 0) # resetting to the original game position\n",
    "        if len(move_scores) == 0:\n",
    "            max_score = 0\n",
    "            max_index = 0\n",
    "        else:\n",
    "            max_score = max(move_scores)\n",
    "            max_index = move_scores.index(max_score)\n",
    "        if return_max_score:\n",
    "            return max_score\n",
    "        return available_moves[max_index]\n",
    "    \n",
    "    def greedy_but_tything_policy(self, game, value_table): \n",
    "    # i.e. 1/10th of the time it makes a random move, the rest of the time it is greedy\n",
    "        if r.random() < self.tythe_rate:\n",
    "            return self.random_policy(game, value_table)\n",
    "        else:\n",
    "            return self.greedy_policy(game, value_table)\n",
    "    \n",
    "    def visualize_value_table(self, game, value_table):\n",
    "        value_grid = np.zeros((game.dim, game.dim))\n",
    "        available_moves = game.available_moves() # i.e. get unfilled positions in grid\n",
    "        for available_move in available_moves:\n",
    "            game.update_state(available_move, self.move) # getting position after potential move is made\n",
    "            encoded_state = self.encode_state(game)\n",
    "            if encoded_state not in value_table.keys():\n",
    "                value_grid[available_move] = np.Infinity\n",
    "            else:\n",
    "                value_grid[available_move] += value_table[encoded_state] # inserting value of move\n",
    "#             print(value_table[encoded_state])\n",
    "            game.update_state(available_move, 0) # resetting to the original game position\n",
    "        value_grid += game.grid\n",
    "        print(value_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000\n",
      "Epoch: 2000\n",
      "Epoch: 3000\n",
      "Epoch: 4000\n",
      "Epoch: 5000\n",
      "Epoch: 6000\n",
      "Epoch: 7000\n",
      "Epoch: 8000\n",
      "Epoch: 9000\n",
      "Epoch: 10000\n",
      "Epoch: 11000\n",
      "Epoch: 12000\n",
      "Epoch: 13000\n",
      "Epoch: 14000\n",
      "Epoch: 15000\n",
      "Epoch: 16000\n",
      "Epoch: 17000\n",
      "Epoch: 18000\n",
      "Epoch: 19000\n",
      "Epoch: 20000\n"
     ]
    }
   ],
   "source": [
    "value_table = {}\n",
    "agents = [Agent(move=1), Agent(move=-1)]\n",
    "# policy = RANDOM_POLICY\n",
    "# policy = GREEDY_POLICY\n",
    "policy = GREEDY_BUT_TYTHING_POLICY\n",
    "num_epochs = 20000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    game = GameBoard(3)\n",
    "    index = 0\n",
    "    inc = 1\n",
    "\n",
    "    while not game.game_over:\n",
    "        move_summary = {}\n",
    "        agent = agents[index]\n",
    "        move_summary['old_state'] = agent.encode_state(game)\n",
    "        agent.take_action(game, value_table, policy)\n",
    "        move_summary['new_state'] = agent.encode_state(game)\n",
    "        agent.game_list.append(move_summary)\n",
    "        game.evaluate_position()\n",
    "        inc *=-1\n",
    "        index +=inc\n",
    "\n",
    "    game.determine_winner()\n",
    "    agents[0].update_value_table_bellman(agents[1], game, value_table)\n",
    "#     agents[0].update_value_table_mc(game, value_table)\n",
    "#     agents[1].update_value_table_mc(game, value_table)\n",
    "    agents[0].reset()\n",
    "    agents[1].reset()\n",
    "    if (i+1)%1000==0:\n",
    "        print(f\"Epoch: {i+1}\")\n",
    "#     print(f\"Epoch: {i+1}\")\n",
    "#     game.view_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5385"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(value_table)\n",
    "# 3**25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf = '+1:0002-1:11'\n",
    "value_table[asdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "1.1131891759548243e-13\n",
      "Row of move:2\n",
      "Column of move:0\n",
      "[[ 4.72625975e-05  5.25487163e-05 -1.05656726e-06]\n",
      " [ 7.45100161e-06  1.00000000e+00  4.86790214e-05]\n",
      " [-1.00000000e+00  5.06762040e-05  6.82345429e-06]]\n",
      "-4.337128933150538e-05\n",
      "[[ 0  1  0]\n",
      " [ 0  1  0]\n",
      " [-1  0  0]]\n",
      "5.25487163190472e-05\n",
      "Row of move:2\n",
      "Column of move:1\n",
      "[[-6.06057551e-01  1.00000000e+00 -6.97531039e-01]\n",
      " [-7.54104907e-01  1.00000000e+00 -3.78748726e-01]\n",
      " [-1.00000000e+00 -1.00000000e+00  3.24119491e-05]]\n",
      "-4.8988702342643536e-05\n",
      "[[ 0  1  0]\n",
      " [ 0  1  0]\n",
      " [-1 -1  1]]\n",
      "3.241194906315419e-05\n",
      "Row of move:0\n",
      "Column of move:0\n",
      "[[-1.00000000e+00  1.00000000e+00 -7.51028506e-01]\n",
      " [ 8.80296872e-06  1.00000000e+00 -6.14384577e-01]\n",
      " [-1.00000000e+00 -1.00000000e+00  1.00000000e+00]]\n",
      "-1.6957514976890145e-05\n",
      "[[-1  1  0]\n",
      " [ 1  1  0]\n",
      " [-1 -1  1]]\n",
      "8.802968719297908e-06\n",
      "Row of move:1\n",
      "Column of move:2\n",
      "[[-1.  1.  0.]\n",
      " [ 1.  1. -1.]\n",
      " [-1. -1.  1.]]\n",
      "0.0\n",
      "[[-1  1  1]\n",
      " [ 1  1 -1]\n",
      " [-1 -1  1]]\n",
      "It's a draw\n"
     ]
    }
   ],
   "source": [
    "## ok, let's see how well the ai does trained on a random policy, then a semi-greedy policy using the same value table\n",
    "# computer goes first\n",
    "game = GameBoard(3)\n",
    "agent = Agent()\n",
    "policy = GREEDY_POLICY\n",
    "\n",
    "while not game.game_over:\n",
    "    agent.take_action(game, value_table, policy)\n",
    "    game.evaluate_position()\n",
    "    if game.game_over:\n",
    "        break\n",
    "    game.view_state()\n",
    "    print(value_table[agent.encode_state(game)])\n",
    "    row = int(input(\"Row of move:\"))\n",
    "    col = int(input(\"Column of move:\"))\n",
    "    game.update_state((row, col), -1)\n",
    "    agent.visualize_value_table(game, value_table)\n",
    "    print(value_table[agent.encode_state(game)])\n",
    "    game.evaluate_position()\n",
    "\n",
    "game.view_state()\n",
    "game.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Row of move:1\n",
      "Column of move:1\n",
      "[[ 0  0 -1]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "Row of move:1\n",
      "Column of move:2\n",
      "[[ 0  0 -1]\n",
      " [-1  1  1]\n",
      " [ 0  0  0]]\n",
      "Row of move:2\n",
      "Column of move:1\n",
      "[[ 0 -1 -1]\n",
      " [-1  1  1]\n",
      " [ 0  1  0]]\n",
      "Row of move:0\n",
      "Column of move:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0b888e28327a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Row of move:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Column of move:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# ok, let's see how well the ai does trained on a random policy, then a semi-greedy policy using the same value table\n",
    "# human goes first\n",
    "game = GameBoard(3)\n",
    "agent = Agent(move=-1)\n",
    "policy = GREEDY_POLICY\n",
    "\n",
    "while not game.game_over:\n",
    "    game.view_state()\n",
    "    row = int(input(\"Row of move:\"))\n",
    "    col = int(input(\"Column of move:\"))\n",
    "    game.update_state((row, col), 1)\n",
    "    game.evaluate_position()\n",
    "    if game.game_over:\n",
    "        break\n",
    "    agent.take_action(game, value_table, policy)\n",
    "    game.evaluate_position()\n",
    "\n",
    "game.view_state()\n",
    "game.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  1],\n",
       "       [ 1,  1,  1],\n",
       "       [-1,  1, -1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(asdf, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Row of move:1\n",
      "Column of move:1\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Row of move:0\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "Row of move:1\n",
      "Column of move:2\n",
      "[[-1  0  0]\n",
      " [ 0  1  1]\n",
      " [ 0  0  0]]\n",
      "Row of move:1\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [-1  1  1]\n",
      " [ 0  0  0]]\n",
      "Row of move:2\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:0\n",
      "Column of move:2\n",
      "[[-1  0 -1]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:0\n",
      "Column of move:1\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:2\n",
      "Column of move:1\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1 -1  0]]\n",
      "Row of move:2\n",
      "Column of move:2\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1 -1  1]]\n",
      "It's a draw\n"
     ]
    }
   ],
   "source": [
    "new_game = GameBoard()\n",
    "move = 1 # 1 being the input for X\n",
    "while not new_game.game_over:\n",
    "    new_game.view_state()\n",
    "    row = int(input(\"Row of move:\"))\n",
    "    col = int(input(\"Column of move:\"))\n",
    "    new_game.update_state((row, col), move)\n",
    "    new_game.evaluate_position()\n",
    "    move *=-1\n",
    "\n",
    "new_game.view_state()\n",
    "new_game.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_game.winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "[[ 1  0  0]\n",
      " [ 0 -1  0]\n",
      " [ 0  0  0]]\n",
      "[[ 1  0  0]\n",
      " [ 1 -1  0]\n",
      " [ 0  0  0]]\n",
      "[[ 1 -1  0]\n",
      " [ 1 -1  0]\n",
      " [ 0  0  0]]\n",
      "[[ 1 -1  0]\n",
      " [ 1 -1  1]\n",
      " [ 0  0  0]]\n",
      "[[ 1 -1 -1]\n",
      " [ 1 -1  1]\n",
      " [ 0  0  0]]\n",
      "[[ 1 -1 -1]\n",
      " [ 1 -1  1]\n",
      " [ 1  0  0]]\n",
      "X won!\n",
      "{'+1:00101220-1:010211': 0.5, '+1:001012-1:010211': -0.2, '+1:001012-1:0111': 0.08000000000000002, '+1:0010-1:0111': -0.03200000000000001, '+1:0010-1:11': 0.012800000000000004, '+1:00-1:11': -0.005120000000000002, '+1:00-1:': 0.0020480000000000008}\n",
      "+1:00-1:: 0.0020480000000000008\n",
      "+1:0010-1:11: 0.012800000000000004\n",
      "+1:001012-1:0111: 0.08000000000000002\n",
      "+1:00101220-1:010211: 0.5\n"
     ]
    }
   ],
   "source": [
    "value_table = {}\n",
    "agents = [Agent(move=1), Agent(move=-1)]\n",
    "policy = GREEDY_BUT_TYTHING_POLICY\n",
    "\n",
    "game = GameBoard(3)\n",
    "index = 0\n",
    "inc = 1\n",
    "\n",
    "while not game.game_over:\n",
    "    move_summary = {}\n",
    "    agent = agents[index]\n",
    "    move_summary['old_state'] = agent.encode_state(game)\n",
    "    agent.take_action(game, value_table, policy)\n",
    "    move_summary['new_state'] = agent.encode_state(game)\n",
    "    agent.game_list.append(move_summary)\n",
    "    game.evaluate_position()\n",
    "    inc *=-1\n",
    "    index +=inc\n",
    "    game.view_state()\n",
    "\n",
    "game.determine_winner()\n",
    "agents[0].update_value_table_bellman(agents[1], game, value_table)\n",
    "\n",
    "game.result()\n",
    "# print(agents[0].game_list)\n",
    "# print(agents[1].game_list)\n",
    "print(value_table)\n",
    "asdf=1\n",
    "for pos in agents[0].game_list:\n",
    "    print(f'{pos[\"new_state\"]}: {value_table[pos[\"new_state\"]]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
