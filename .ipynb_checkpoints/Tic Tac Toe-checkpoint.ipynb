{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exercise in reinforcement learning.  An <b>agent</b> (our AI) makes moves in an <b>environment</b> (our gameboard) using a <b>policy</b> to determine the best move in a given <b>state</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment for our agent to interact with.  A gameboard for tic tac toe.\n",
    "class GameBoard:\n",
    "    def __init__(self, dim=3):\n",
    "        self.grid = np.zeros((dim, dim), dtype=np.int)\n",
    "        self.dim = dim\n",
    "        self.game_over = False\n",
    "        self.game_has_winner = False\n",
    "        self.outcome = 0 #default value to be replaced by 1 for \"X\" or -1 for \"O\"\n",
    "    \n",
    "    def available_moves(self):\n",
    "        result = np.where(self.grid == 0)\n",
    "        listOfCoordinates = list(zip(result[0], result[1]))\n",
    "        return listOfCoordinates\n",
    "\n",
    "    #enter a move, 1 being \"X\", -1 being \"O\"\n",
    "    def update_state(self, coords, val):\n",
    "        self.grid[coords] = val\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.grid\n",
    "    \n",
    "    def view_state(self):\n",
    "        print(self.grid)\n",
    "        \n",
    "    def evaluate_position(self):\n",
    "        # first check if there is a winner\n",
    "        # if a row, column, or diagonal adds up to dim or -dim, it mean X or O has won, respectively\n",
    "        row_comp = np.any(np.abs(np.sum(self.grid, axis = 0)) == self.dim)\n",
    "        col_comp = np.any(np.abs(np.sum(self.grid, axis = 1)) == self.dim)\n",
    "        diag_comp = np.abs(np.trace(self.grid)) == self.dim\n",
    "        anti_diag_comp = np.abs(np.trace(np.flipud(self.grid))) == self.dim\n",
    "        if row_comp or col_comp or diag_comp or anti_diag_comp:\n",
    "            self.game_over = True\n",
    "            self.game_has_winner = True\n",
    "        # check if the grid is filled\n",
    "        # a draw, since, neither player has won by the above criteria\n",
    "        if not np.any(self.grid == 0):\n",
    "            self.game_over = True\n",
    "            self.game_has_winner = False\n",
    "    \n",
    "    def determine_winner(self):\n",
    "        self.evaluate_position()\n",
    "        if self.game_has_winner:\n",
    "            row_comp = np.any(np.sum(self.grid, axis = 0) == self.dim)\n",
    "            col_comp = np.any(np.sum(self.grid, axis = 1) == self.dim)\n",
    "            diag_comp = np.trace(self.grid) == self.dim\n",
    "            anti_diag_comp = np.trace(np.flipud(self.grid)) == self.dim\n",
    "            if row_comp or col_comp or diag_comp or anti_diag_comp:\n",
    "                self.outcome = 1\n",
    "            else:\n",
    "                self.outcome = -1\n",
    "    \n",
    "    def result(self):\n",
    "        self.determine_winner()\n",
    "        if self.game_over and self.game_has_winner:\n",
    "            if self.outcome == 1:\n",
    "                print(\"X won!\")\n",
    "            else:\n",
    "                print(\"O won!\")\n",
    "        elif self.game_over:\n",
    "            print(\"It's a draw\")\n",
    "        else:\n",
    "            print(\"Game in progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our agent that will be playing the game\n",
    "class Agent:\n",
    "    def __init__(self, epsilon=0.1, discount_factor=0.8, learning_rate=0.5, move=1): # move=1 for X, -1 for O\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.move = move\n",
    "        self.game_list = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game_list = []\n",
    "        \n",
    "    def take_action(self, game, value_table, policy):\n",
    "        action = policy(game, value_table)\n",
    "        game.update_state(action, self.move)\n",
    "        \n",
    "    def calculate_rewards(self, game):\n",
    "        reward = self.move * game.outcome\n",
    "        for i in range(1, len(self.game_list)+1):\n",
    "            self.game_list[-i]['reward'] = self.discount_factor**(i-1) * reward\n",
    "            \n",
    "    def update_value_table(self, game, value_table):\n",
    "        self.calculate_rewards(game)\n",
    "        for item in self.game_list:\n",
    "            key = item['new_state']\n",
    "            reward = item['reward']\n",
    "            if key not in value_table.keys():\n",
    "                value_table[key] = 0\n",
    "            value_table[key] += self.learning_rate * reward\n",
    "        \n",
    "    # value table is a dictionary of states and values.  game.get_state is a numpy array that can't be used\n",
    "    # as a dictionary key.  We encode the state as a string for it to be usable.  If the agent plays O's,\n",
    "    # we will multiply the matrix by move (-1) first, so that the state is the same for both X and O\n",
    "    def encode_state(self, game):\n",
    "        state = self.move * game.get_state()\n",
    "        xs = np.where(state == 1)\n",
    "        os = np.where(state == -1)\n",
    "        xcoords = list(zip(xs[0], xs[1]))\n",
    "        ocoords = list(zip(os[0], os[1]))\n",
    "        encoded_state = '+1:'\n",
    "        for tup in xcoords:\n",
    "            for a in tup:\n",
    "                encoded_state +=str(a)\n",
    "        encoded_state +='-1:'\n",
    "        for tup in ocoords:\n",
    "            for a in tup:\n",
    "                encoded_state +=str(a)\n",
    "        return encoded_state\n",
    "    \n",
    "    def random_policy(self, game, value_table):\n",
    "    return r.choice(game.available_moves())\n",
    "\n",
    "    def greedy_policy(self, game, value_table):\n",
    "        available_moves = game.available_moves()\n",
    "        move_scores = []\n",
    "        for available_move in available_moves:\n",
    "            game.update_state(available_move, self.move)\n",
    "            encoded_state = self.encode_state(game)\n",
    "            if encoded_state not in value_table.keys():\n",
    "                value_table[encoded_state] = 0\n",
    "                move_scores.append(0)\n",
    "            else:\n",
    "                move_scores.append(value_table[encoded_state])\n",
    "            game.update_state(available_move, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100\n",
      "Epoch: 200\n",
      "Epoch: 300\n",
      "Epoch: 400\n",
      "Epoch: 500\n",
      "Epoch: 600\n",
      "Epoch: 700\n",
      "Epoch: 800\n",
      "Epoch: 900\n",
      "Epoch: 1000\n"
     ]
    }
   ],
   "source": [
    "value_table = {}\n",
    "agents = [Agent(move=1), Agent(move=-1)]\n",
    "policy = random_policy\n",
    "num_epochs = 1000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    game = GameBoard()\n",
    "    index = 0\n",
    "    inc = 1\n",
    "\n",
    "    while not game.game_over:\n",
    "        move_summary = {}\n",
    "        agent = agents[index]\n",
    "        move_summary['old_state'] = agent.encode_state(game)\n",
    "        agent.take_action(game, value_table, policy)\n",
    "        move_summary['new_state'] = agent.encode_state(game)\n",
    "        agent.game_list.append(move_summary)\n",
    "        game.evaluate_position()\n",
    "        inc *=-1\n",
    "        index +=inc\n",
    "\n",
    "    game.determine_winner()\n",
    "    agents[0].update_value_table(game, value_table)\n",
    "    agents[1].update_value_table(game, value_table)\n",
    "    agents[0].reset()\n",
    "    agents[1].reset()\n",
    "    if (i+1)%100==0:\n",
    "        print(f\"Epoch: {i+1}\")\n",
    "#     game.view_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3189"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Row of move:1\n",
      "Column of move:1\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Row of move:0\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "Row of move:1\n",
      "Column of move:2\n",
      "[[-1  0  0]\n",
      " [ 0  1  1]\n",
      " [ 0  0  0]]\n",
      "Row of move:1\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [-1  1  1]\n",
      " [ 0  0  0]]\n",
      "Row of move:2\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:0\n",
      "Column of move:2\n",
      "[[-1  0 -1]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:0\n",
      "Column of move:1\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:2\n",
      "Column of move:1\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1 -1  0]]\n",
      "Row of move:2\n",
      "Column of move:2\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1 -1  1]]\n",
      "It's a draw\n"
     ]
    }
   ],
   "source": [
    "new_game = GameBoard()\n",
    "move = 1 # 1 being the input for X\n",
    "while not new_game.game_over:\n",
    "    new_game.view_state()\n",
    "    row = int(input(\"Row of move:\"))\n",
    "    col = int(input(\"Column of move:\"))\n",
    "    new_game.update_state((row, col), move)\n",
    "    new_game.evaluate_position()\n",
    "    move *=-1\n",
    "\n",
    "new_game.view_state()\n",
    "new_game.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_game.winner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
