{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an exercise in reinforcement learning.  An <b>agent</b> (our AI) makes moves in an <b>environment</b> (our gameboard) using a <b>policy</b> to determine the best move in a given <b>state</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "# defining string constants for specifying policies to use\n",
    "RANDOM_POLICY = \"random_policy\"\n",
    "GREEDY_POLICY = \"greedy_policy\"\n",
    "GREEDY_BUT_TYTHING_POLICY = \"greedy_but_tything_policy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the environment for our agent to interact with.  A gameboard for tic tac toe.\n",
    "class GameBoard:\n",
    "    def __init__(self, dim=3):\n",
    "        self.grid = np.zeros((dim, dim), dtype=np.int)\n",
    "        self.dim = dim\n",
    "        self.game_over = False\n",
    "        self.game_has_winner = False\n",
    "        self.outcome = 0 #default value to be replaced by 1 for \"X\" or -1 for \"O\"\n",
    "    \n",
    "    def available_moves(self):\n",
    "        result = np.where(self.grid == 0)\n",
    "        listOfCoordinates = list(zip(result[0], result[1]))\n",
    "        return listOfCoordinates\n",
    "\n",
    "    #enter a move, 1 being \"X\", -1 being \"O\"\n",
    "    def update_state(self, coords, val):\n",
    "        self.grid[coords] = val\n",
    "    \n",
    "    def get_state(self):\n",
    "        return self.grid\n",
    "    \n",
    "    def view_state(self):\n",
    "        print(self.grid)\n",
    "        \n",
    "    def evaluate_position(self):\n",
    "        # first check if there is a winner\n",
    "        # if a row, column, or diagonal adds up to dim or -dim, it mean X or O has won, respectively\n",
    "        row_comp = np.any(np.abs(np.sum(self.grid, axis = 1)) == self.dim)\n",
    "        col_comp = np.any(np.abs(np.sum(self.grid, axis = 0)) == self.dim)\n",
    "        diag_comp = np.abs(np.trace(self.grid)) == self.dim\n",
    "        anti_diag_comp = np.abs(np.trace(np.flipud(self.grid))) == self.dim\n",
    "        if row_comp or col_comp or diag_comp or anti_diag_comp:\n",
    "            self.game_over = True\n",
    "            self.game_has_winner = True\n",
    "        # check if the grid is filled\n",
    "        # a draw, since, neither player has won by the above criteria\n",
    "        elif not np.any(self.grid == 0):\n",
    "            self.game_over = True\n",
    "            self.game_has_winner = False\n",
    "    \n",
    "    def determine_winner(self):\n",
    "        self.evaluate_position()\n",
    "        if self.game_has_winner:\n",
    "            row_comp = np.any(np.sum(self.grid, axis = 1) == self.dim)\n",
    "            col_comp = np.any(np.sum(self.grid, axis = 0) == self.dim)\n",
    "            diag_comp = np.trace(self.grid) == self.dim\n",
    "            anti_diag_comp = np.trace(np.flipud(self.grid)) == self.dim\n",
    "            if row_comp or col_comp or diag_comp or anti_diag_comp:\n",
    "                self.outcome = 1\n",
    "            else:\n",
    "                self.outcome = -1\n",
    "    \n",
    "    def result(self):\n",
    "        self.determine_winner()\n",
    "        if self.game_over and self.game_has_winner:\n",
    "            if self.outcome == 1:\n",
    "                print(\"X won!\")\n",
    "            else:\n",
    "                print(\"O won!\")\n",
    "        elif self.game_over:\n",
    "            print(\"It's a draw\")\n",
    "        else:\n",
    "            print(\"Game in progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our agent that will be playing the game\n",
    "class Agent:\n",
    "    def __init__(self, epsilon=0.1, discount_factor=0.8, learning_rate=0.1, tythe_rate = 0.1,\n",
    "                 move=1): # move=1 for X, -1 for O\n",
    "        self.epsilon = epsilon\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tythe_rate = tythe_rate\n",
    "        self.move = move\n",
    "        self.game_list = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.game_list = []\n",
    "        \n",
    "    def take_action(self, game, value_table, policy_string):\n",
    "        if policy_string == RANDOM_POLICY:\n",
    "            policy = self.random_policy\n",
    "        elif policy_string == GREEDY_POLICY:\n",
    "            policy = self.greedy_policy\n",
    "        elif policy_string == GREEDY_BUT_TYTHING_POLICY:\n",
    "            policy = self.greedy_but_tything_policy\n",
    "        action = policy(game, value_table)\n",
    "        game.update_state(action, self.move)\n",
    "        \n",
    "    def calculate_rewards(self, game):\n",
    "        reward = self.move * game.outcome\n",
    "        for i in range(1, len(self.game_list)+1):\n",
    "            self.game_list[-i]['reward'] = self.discount_factor**(i-1) * reward\n",
    "            \n",
    "    def update_value_table(self, game, value_table):\n",
    "        self.calculate_rewards(game)\n",
    "        for item in self.game_list:\n",
    "            key = item['new_state']\n",
    "            reward = item['reward']\n",
    "            if key not in value_table.keys():\n",
    "                value_table[key] = 0\n",
    "            value_table[key] += self.learning_rate * (reward - value_table[key])\n",
    "        \n",
    "    # value table is a dictionary of states and values.  game.get_state is a numpy array that can't be used\n",
    "    # as a dictionary key.  We encode the state as a string for it to be usable.  If the agent plays O's,\n",
    "    # we will multiply the matrix by move (-1) first, so that the state is the same for both X and O\n",
    "    def encode_state(self, game):\n",
    "        state = self.move * game.get_state()\n",
    "        xs = np.where(state == 1)\n",
    "        os = np.where(state == -1)\n",
    "        xcoords = list(zip(xs[0], xs[1]))\n",
    "        ocoords = list(zip(os[0], os[1]))\n",
    "        encoded_state = '+1:'\n",
    "        for tup in xcoords:\n",
    "            for a in tup:\n",
    "                encoded_state +=str(a)\n",
    "        encoded_state +='-1:'\n",
    "        for tup in ocoords:\n",
    "            for a in tup:\n",
    "                encoded_state +=str(a)\n",
    "        return encoded_state\n",
    "    \n",
    "    def random_policy(self, game, value_table):\n",
    "        return r.choice(game.available_moves())\n",
    "\n",
    "    def greedy_policy(self, game, value_table):\n",
    "        available_moves = game.available_moves() # i.e. get unfilled positions in grid\n",
    "        move_scores = []\n",
    "        for available_move in available_moves:\n",
    "            game.update_state(available_move, self.move) # getting position after potential move is made\n",
    "            encoded_state = self.encode_state(game)\n",
    "            if encoded_state not in value_table.keys():\n",
    "                value_table[encoded_state] = 0\n",
    "            move_scores.append(value_table[encoded_state]) # getting value of new position\n",
    "            game.update_state(available_move, 0) # resetting to the original game position\n",
    "        max_score = max(move_scores)\n",
    "        max_index = move_scores.index(max_score)\n",
    "        return available_moves[max_index]\n",
    "    \n",
    "    def greedy_but_tything_policy(self, game, value_table): \n",
    "    # i.e. 1/10th of the time it makes a random move, the rest of the time it is greedy\n",
    "        if r.random() < self.tythe_rate:\n",
    "            return self.random_policy(game, value_table)\n",
    "        else:\n",
    "            return self.greedy_policy(game, value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Epoch: 10000\n",
      "Random Epoch: 20000\n",
      "Random Epoch: 30000\n",
      "Random Epoch: 40000\n",
      "Random Epoch: 50000\n",
      "Random Epoch: 60000\n",
      "Random Epoch: 70000\n",
      "Random Epoch: 80000\n",
      "Random Epoch: 90000\n",
      "Random Epoch: 100000\n",
      "Random Epoch: 110000\n",
      "Random Epoch: 120000\n",
      "Random Epoch: 130000\n",
      "Random Epoch: 140000\n",
      "Random Epoch: 150000\n",
      "Random Epoch: 160000\n",
      "Random Epoch: 170000\n",
      "Random Epoch: 180000\n",
      "Random Epoch: 190000\n",
      "Random Epoch: 200000\n",
      "Random Epoch: 210000\n",
      "Random Epoch: 220000\n",
      "Random Epoch: 230000\n",
      "Random Epoch: 240000\n",
      "Random Epoch: 250000\n",
      "Random Epoch: 260000\n",
      "Random Epoch: 270000\n",
      "Random Epoch: 280000\n",
      "Random Epoch: 290000\n",
      "Random Epoch: 300000\n",
      "Random Epoch: 310000\n",
      "Random Epoch: 320000\n",
      "Random Epoch: 330000\n",
      "Random Epoch: 340000\n",
      "Random Epoch: 350000\n",
      "Random Epoch: 360000\n",
      "Random Epoch: 370000\n",
      "Random Epoch: 380000\n",
      "Random Epoch: 390000\n",
      "Random Epoch: 400000\n",
      "Random Epoch: 410000\n",
      "Random Epoch: 420000\n",
      "Random Epoch: 430000\n",
      "Random Epoch: 440000\n",
      "Random Epoch: 450000\n",
      "Random Epoch: 460000\n",
      "Random Epoch: 470000\n",
      "Random Epoch: 480000\n",
      "Random Epoch: 490000\n",
      "Random Epoch: 500000\n",
      "Random Epoch: 510000\n",
      "Random Epoch: 520000\n",
      "Random Epoch: 530000\n",
      "Random Epoch: 540000\n",
      "Random Epoch: 550000\n",
      "Random Epoch: 560000\n",
      "Random Epoch: 570000\n",
      "Random Epoch: 580000\n",
      "Random Epoch: 590000\n",
      "Random Epoch: 600000\n",
      "Random Epoch: 610000\n",
      "Random Epoch: 620000\n",
      "Random Epoch: 630000\n",
      "Random Epoch: 640000\n",
      "Random Epoch: 650000\n",
      "Random Epoch: 660000\n",
      "Random Epoch: 670000\n",
      "Random Epoch: 680000\n",
      "Random Epoch: 690000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f70809d80573>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmove_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mmove_summary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'old_state'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mmove_summary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'new_state'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-887381d94f7b>\u001b[0m in \u001b[0;36mencode_state\u001b[1;34m(self, game)\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mencoded_state\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mencoded_state\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;34m'-1:'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mtup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mocoords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mencoded_state\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "value_table = {}\n",
    "agents = [Agent(move=1), Agent(move=-1)]\n",
    "policy = RANDOM_POLICY\n",
    "# policy = GREEDY_POLICY\n",
    "# policy = GREEDY_BUT_TYTHING_POLICY\n",
    "num_epochs = 1000000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    game = GameBoard(5)\n",
    "    index = 0\n",
    "    inc = 1\n",
    "\n",
    "    while not game.game_over:\n",
    "        move_summary = {}\n",
    "        agent = agents[index]\n",
    "        move_summary['old_state'] = agent.encode_state(game)\n",
    "        agent.take_action(game, value_table, policy)\n",
    "        move_summary['new_state'] = agent.encode_state(game)\n",
    "        agent.game_list.append(move_summary)\n",
    "        game.evaluate_position()\n",
    "        inc *=-1\n",
    "        index +=inc\n",
    "\n",
    "    game.determine_winner()\n",
    "    agents[0].update_value_table(game, value_table)\n",
    "    agents[1].update_value_table(game, value_table)\n",
    "    agents[0].reset()\n",
    "    agents[1].reset()\n",
    "    if (i+1)%10000==0:\n",
    "        print(f\"Random Epoch: {i+1}\")\n",
    "#     print(f\"Epoch: {i+1}\")\n",
    "#     game.view_state()\n",
    "\n",
    "\n",
    "\n",
    "# now that we've prepopulated the value table with a random policy, let's see if we can make it better by training it \n",
    "# with the semi-greedy tything policy\n",
    "# value table will not be re-initialized, so that we can use its existing values\n",
    "\n",
    "agents = [Agent(move=1), Agent(move=-1)]\n",
    "policy = GREEDY_BUT_TYTHING_POLICY\n",
    "num_epochs = 1000000\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    game = GameBoard(5)\n",
    "    index = 0\n",
    "    inc = 1\n",
    "\n",
    "    while not game.game_over:\n",
    "        move_summary = {}\n",
    "        agent = agents[index]\n",
    "        move_summary['old_state'] = agent.encode_state(game)\n",
    "        agent.take_action(game, value_table, policy)\n",
    "        move_summary['new_state'] = agent.encode_state(game)\n",
    "        agent.game_list.append(move_summary)\n",
    "        game.evaluate_position()\n",
    "        inc *=-1\n",
    "        index +=inc\n",
    "\n",
    "    game.determine_winner()\n",
    "    agents[0].update_value_table(game, value_table)\n",
    "    agents[1].update_value_table(game, value_table)\n",
    "    agents[0].reset()\n",
    "    agents[1].reset()\n",
    "    if (i+1)%10000==0:\n",
    "        print(f\"Semi-Greedy Epoch: {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13347668"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(value_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "Row of move:2\n",
      "Column of move:2\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0  1]]\n",
      "Row of move:2\n",
      "Column of move:1\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1 -1 -1  0  0]\n",
      " [ 0  1  0  0  0]\n",
      " [ 0  0  0  0  1]]\n",
      "Row of move:3\n",
      "Column of move:2\n",
      "[[ 1  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1 -1 -1  0  0]\n",
      " [ 0  1 -1  0  0]\n",
      " [ 0  0  0  0  1]]\n",
      "Row of move:4\n",
      "Column of move:2\n",
      "[[ 1  1  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1 -1 -1  0  0]\n",
      " [ 0  1 -1  0  0]\n",
      " [ 0  0 -1  0  1]]\n",
      "Row of move:0\n",
      "Column of move:2\n",
      "[[ 1  1 -1  1  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 1 -1 -1  0  0]\n",
      " [ 0  1 -1  0  0]\n",
      " [ 0  0 -1  0  1]]\n",
      "Row of move:1\n",
      "Column of move:2\n",
      "[[ 1  1 -1  1  0]\n",
      " [ 0  0 -1  0  0]\n",
      " [ 1 -1 -1  0  0]\n",
      " [ 0  1 -1  0  0]\n",
      " [ 0  0 -1  0  1]]\n",
      "O won!\n"
     ]
    }
   ],
   "source": [
    "# ok, let's see how well the ai does trained on a random policy, then a semi-greedy policy using the same value table\n",
    "# computer goes first\n",
    "game = GameBoard(5)\n",
    "agent = Agent()\n",
    "policy = GREEDY_POLICY\n",
    "\n",
    "while not game.game_over:\n",
    "    agent.take_action(game, value_table, policy)\n",
    "    game.evaluate_position()\n",
    "    if game.game_over:\n",
    "        break\n",
    "    game.view_state()\n",
    "    row = int(input(\"Row of move:\"))\n",
    "    col = int(input(\"Column of move:\"))\n",
    "    game.update_state((row, col), -1)\n",
    "    game.evaluate_position()\n",
    "\n",
    "game.view_state()\n",
    "game.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Row of move:1\n",
      "Column of move:1\n",
      "[[ 0  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0 -1]]\n",
      "Row of move:2\n",
      "Column of move:1\n",
      "[[ 0 -1  0]\n",
      " [ 0  1  0]\n",
      " [ 0  1 -1]]\n",
      "Row of move:0\n",
      "Column of move:2\n",
      "[[ 0 -1  1]\n",
      " [ 0  1  0]\n",
      " [-1  1 -1]]\n",
      "Row of move:1\n",
      "Column of move:0\n",
      "[[ 0 -1  1]\n",
      " [ 1  1 -1]\n",
      " [-1  1 -1]]\n",
      "Row of move:0\n",
      "Column of move:0\n",
      "[[ 1 -1  1]\n",
      " [ 1  1 -1]\n",
      " [-1  1 -1]]\n",
      "It's a draw\n"
     ]
    }
   ],
   "source": [
    "# ok, let's see how well the ai does trained on a random policy, then a semi-greedy policy using the same value table\n",
    "# human goes first\n",
    "game = GameBoard(5)\n",
    "agent = Agent(move=-1)\n",
    "policy = GREEDY_POLICY\n",
    "\n",
    "while not game.game_over:\n",
    "    game.view_state()\n",
    "    row = int(input(\"Row of move:\"))\n",
    "    col = int(input(\"Column of move:\"))\n",
    "    game.update_state((row, col), 1)\n",
    "    game.evaluate_position()\n",
    "    if game.game_over:\n",
    "        break\n",
    "    agent.take_action(game, value_table, policy)\n",
    "    game.evaluate_position()\n",
    "\n",
    "game.view_state()\n",
    "game.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  1],\n",
       "       [ 1,  1,  1],\n",
       "       [-1,  1, -1]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(asdf, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Row of move:1\n",
      "Column of move:1\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]]\n",
      "Row of move:0\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0  0]]\n",
      "Row of move:1\n",
      "Column of move:2\n",
      "[[-1  0  0]\n",
      " [ 0  1  1]\n",
      " [ 0  0  0]]\n",
      "Row of move:1\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [-1  1  1]\n",
      " [ 0  0  0]]\n",
      "Row of move:2\n",
      "Column of move:0\n",
      "[[-1  0  0]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:0\n",
      "Column of move:2\n",
      "[[-1  0 -1]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:0\n",
      "Column of move:1\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1  0  0]]\n",
      "Row of move:2\n",
      "Column of move:1\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1 -1  0]]\n",
      "Row of move:2\n",
      "Column of move:2\n",
      "[[-1  1 -1]\n",
      " [-1  1  1]\n",
      " [ 1 -1  1]]\n",
      "It's a draw\n"
     ]
    }
   ],
   "source": [
    "new_game = GameBoard()\n",
    "move = 1 # 1 being the input for X\n",
    "while not new_game.game_over:\n",
    "    new_game.view_state()\n",
    "    row = int(input(\"Row of move:\"))\n",
    "    col = int(input(\"Column of move:\"))\n",
    "    new_game.update_state((row, col), move)\n",
    "    new_game.evaluate_position()\n",
    "    move *=-1\n",
    "\n",
    "new_game.view_state()\n",
    "new_game.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_game.winner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
